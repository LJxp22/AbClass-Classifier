import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, cross_validate, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    recall_score, precision_score, f1_score, 
    confusion_matrix, accuracy_score, classification_report
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
import matplotlib.pyplot as plt
import shap

def load_and_preprocess_data(file_path):
    """Load and preprocess dataset"""
    try:
        # Load data
        data = pd.read_csv(file_path, encoding='gbk')
        print(f"Data loaded successfully, original shape: {data.shape}")
        
        # Data preprocessing
        data.drop("V1", axis=1, inplace=True)
        numeric_columns = data.columns[1:]
        data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')
        data.dropna(inplace=True)
        data['label'] = data['label'].astype('float32')
        
        print(f"Data shape after preprocessing: {data.shape}")
        return data
    except Exception as e:
        print(f"Data processing error: {e}")
        raise

def prepare_features_labels(data):
    """Prepare features and labels"""
    X = data.drop('label', axis=1).values
    y = data['label'].values
    return X, y

def split_and_scale_data(X, y, test_size=0.2, random_state=42):
    """Split data into train/test sets and standardize features"""
    # Split dataset
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train, y_test

def create_base_estimators():
    """Create base classifiers"""
    clf1 = LogisticRegression(
        max_iter=3000, C=0.1, random_state=1412, n_jobs=8
    )
    
    clf2 = RandomForestClassifier(
        n_estimators=165, 
        max_features="sqrt", 
        max_depth=4, 
        min_samples_leaf=4, 
        random_state=1412,
        n_jobs=8
    )
    
    clf3 = svm.SVC(C=10, probability=True)
    
    clf4 = KNeighborsClassifier(n_neighbors=10, n_jobs=8)
    
    estimators = [
        ("Logistic Regression", clf1), 
        ("Random Forest", clf2),
        ("SVM", clf3), 
        ("KNN", clf4)
    ]
    
    return estimators

def create_stacking_model(estimators):
    """Create Stacking ensemble model"""
    final_estimator = RandomForestClassifier(
        n_estimators=100,
        min_impurity_decrease=0.0025,
        random_state=420,
        n_jobs=8
    )
    
    stacking_clf = StackingClassifier(
        estimators=estimators,
        final_estimator=final_estimator,
        n_jobs=8
    )
    
    return stacking_clf

def evaluate_model(model, X_train, X_test, y_train, y_test, cv=5):
    """Evaluate model performance"""
    # Cross-validation
    cv_results = cross_validate(
        model, X_train, y_train, 
        cv=KFold(n_splits=cv, shuffle=True, random_state=1412),
        scoring="accuracy",
        n_jobs=-1,
        return_train_score=True
    )
    
    # Train model and evaluate on test set
    model.fit(X_train, y_train)
    test_score = model.score(X_test, y_test)
    
    print("\n----- Model Evaluation Results -----")
    print(f"Training set average accuracy: {cv_results['train_score'].mean():.4f}")
    print(f"Cross-validation average accuracy: {cv_results['test_score'].mean():.4f}")
    print(f"Test set accuracy: {test_score:.4f}")
    
    return model

def evaluate_individual_models(estimators, X_train, X_test, y_train, y_test, cv=5):
    """Evaluate individual base models"""
    print("\n----- Base Model Evaluation Results -----")
    
    for name, estimator in estimators:
        # Cross-validation
        cv_results = cross_validate(
            estimator, X_train, y_train,
            cv=KFold(n_splits=cv, shuffle=True, random_state=1400),
            scoring=("accuracy", "precision_weighted", "recall_weighted", "f1_weighted"),
            n_jobs=-1,
            return_train_score=True
        )
        
        # Train and predict
        estimator.fit(X_train, y_train)
        y_pred = estimator.predict(X_test)
        
        # Calculate test set metrics
        test_accuracy = accuracy_score(y_test, y_pred)
        test_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        test_recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        test_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Print results
        print(f'\n{name}:')
        print(f'  Test Accuracy: {test_accuracy:.4f}')
        print(f'  Test F1 Score: {test_f1:.4f}')
        print(f'  Test Precision: {test_precision:.4f}')
        print(f'  Test Recall: {test_recall:.4f}')
        
        # Print classification report
        print("  Detailed Classification Report:")
        print(classification_report(y_test, y_pred, zero_division=0))

def plot_shap_importance(model, X_test, feature_names, class_names):
    """Plot SHAP feature importance"""
    try:
        # Create SHAP explainer
        if hasattr(model, 'predict_proba'):
            explainer = shap.KernelExplainer(model.predict_proba, X_test[:50])
            shap_values = explainer.shap_values(X_test[:50])
        else:
            explainer = shap.KernelExplainer(model.predict, X_test[:50])
            shap_values = explainer.shap_values(X_test[:50])
        
        # Plot feature importance
        plt.figure(figsize=(12, 8))
        if isinstance(shap_values, list):  # Multi-class classification
            shap.summary_plot(shap_values, X_test[:50], feature_names=feature_names, class_names=class_names)
        else:  # Binary classification
            shap.summary_plot(shap_values, X_test[:50], feature_names=feature_names)
        
        plt.tight_layout()
        plt.show()
        
    except Exception as e:
        print(f"SHAP visualization failed: {e}")
        print("Please ensure the 'shap' library is installed or try reducing the sample size")

def main(file_path):
    """Main function: Execute the complete model training and evaluation pipeline"""
    try:
        # 1. Load and preprocess data
        data = load_and_preprocess_data(file_path)
        
        # 2. Prepare features and labels
        X, y = prepare_features_labels(data)
        
        # 3. Split dataset and standardize features
        X_train, X_test, y_train, y_test = split_and_scale_data(X, y)
        
        # 4. Create base models
        base_estimators = create_base_estimators()
        
        # 5. Create Stacking ensemble model
        stacking_model = create_stacking_model(base_estimators)
        
        # 6. Evaluate ensemble model
        trained_model = evaluate_model(stacking_model, X_train, X_test, y_train, y_test)
        
        # 7. Evaluate individual models
        evaluate_individual_models(base_estimators, X_train, X_test, y_train, y_test)
        
        # 8. Plot feature importance
        feature_names = data.columns[1:].tolist()
        class_names = ['Anti-Dengue', 'Anti-Influenza', 'Anti-Tetanus', 'Anti-SARS-CoV-2', 'Anti-Tuberculosis']
        plot_shap_importance(trained_model, X_test, feature_names, class_names)
        
        return trained_model
        
    except Exception as e:
        print(f"Pipeline execution error: {e}")
        raise

if __name__ == "__main__":
    # Replace with actual data file path
    file_path = "your_data.csv"
    main(file_path)
